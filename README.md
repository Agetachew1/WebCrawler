# WebCrawler
This project aims to create a multithreaded web crawler tailored to explore web pages by following linked URLs. It begins with a specified URL and systematically delves deeper into the site, limited by a predefined maximum depth to prevent infinite crawling. 
This project aims to create a multithreaded web crawler tailored to explore web pages by following linked URLs. It begins with a specified URL and systematically delves deeper into the site, limited by a predefined maximum depth to prevent infinite crawling. To optimize efficiency, the crawler remembers visited URLs, preventing redundant exploration of the same pages. Output logs will detail each crawler's unique visits to URLs. Employing multiple threads enhances the crawler's performance by enabling simultaneous exploration of multiple web pages, significantly improving efficiency in navigating through the web.
